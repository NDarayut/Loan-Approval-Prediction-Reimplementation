{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into features (X) and target (y)\n",
    "X = data_scaled.drop('Loan_Status', axis=1)  # Features\n",
    "y = data_scaled['Loan_Status']  # Target variable\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Apply SMOTE to the training set only\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=0)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train set Approved loan: \", (y_train == 1).sum())\n",
    "print(\"Train set Rejected loan: \", (y_train == 0).sum())\n",
    "print(\"Test set Approved loan: \", (y_test == 1).sum())\n",
    "print(\"Test set Rejected loan: \", (y_test == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train set: \", len(y_train))\n",
    "print(\"Test set: \", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying log transformation to normalize data and reduce skewness in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Copy for comparison\n",
    "train_data_old = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_data[\"ApplicantIncome\"] = np.log1p(train_data[\"ApplicantIncome\"])\n",
    "train_data[\"CoapplicantIncome\"] = np.log1p(train_data[\"CoapplicantIncome\"])\n",
    "train_data[\"LoanAmount\"] = np.log1p(train_data[\"LoanAmount\"])\n",
    "\n",
    "\n",
    "test_data[\"ApplicantIncome\"] = np.log1p(test_data[\"ApplicantIncome\"])\n",
    "test_data[\"CoapplicantIncome\"] = np.log1p(test_data[\"CoapplicantIncome\"])\n",
    "test_data[\"LoanAmount\"] = np.log1p(test_data[\"LoanAmount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot histograms before and after transformation\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Original data\n",
    "ax[0].hist(train_data_old['ApplicantIncome'], bins=30, color='blue', alpha=0.7)\n",
    "ax[0].set_title('Original ApplicantIncome')\n",
    "\n",
    "# After log transformation\n",
    "ax[1].hist(train_data['ApplicantIncome'], bins=30, color='green', alpha=0.7)\n",
    "ax[1].set_title('Log Transformed ApplicantIncome')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot histograms before and after transformation\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Original data\n",
    "ax[0].hist(train_data_old['CoapplicantIncome'], bins=30, color='blue', alpha=0.7)\n",
    "ax[0].set_title('Original CoapplicantIncome')\n",
    "\n",
    "# After log transformation\n",
    "ax[1].hist(train_data['CoapplicantIncome'], bins=30, color='green', alpha=0.7)\n",
    "ax[1].set_title('Log Transformed CoapplicantIncome')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot histograms before and after transformation\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Original data\n",
    "ax[0].hist(train_data_old['LoanAmount'], bins=30, color='blue', alpha=0.7)\n",
    "ax[0].set_title('Original LoanAmount')\n",
    "\n",
    "# After log transformation\n",
    "ax[1].hist(train_data['LoanAmount'], bins=30, color='green', alpha=0.7)\n",
    "ax[1].set_title('Log Transformed LoanAmount')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler on the data and transform it\n",
    "data_scaled = scaler.fit_transform(train_data)\n",
    "\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=train_data.columns)\n",
    "\n",
    "data_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Approved loan: \", len(data_scaled[data_scaled[\"Loan_Status\"] == 1.]))\n",
    "print(\"Rejected loan: \", len(data_scaled[data_scaled[\"Loan_Status\"] == 0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Count Loan_Status values\n",
    "loan_status_counts = data_scaled['Loan_Status'].value_counts()\n",
    "\n",
    "# Extract data for the pie chart\n",
    "labels = loan_status_counts.index  # Categories (Y, N)\n",
    "sizes = loan_status_counts.values  # Count of each category\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.figure(figsize=(6, 6))  # Adjust size of the figure\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=['lightblue', 'lightcoral'])\n",
    "plt.title('Loan Status Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is heavily imabalance where most of the data is approved loan. According to the paper, the author uses 2 data augmentation technique, SMOTE, and another method which include training a simple machine learning model using the available data. After that, they utilized user-selected data, which closely resemble the available data, to evaluate the model and predict the corresponding class labels.\n",
    "\n",
    "However, since the author failed to mention what machine learning model they used to trained on the available data and how many user-selected data they used, we will only be using SMOTE to augment the data after splitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
